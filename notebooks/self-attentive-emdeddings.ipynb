{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from self_attentive_train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'model_path': '../checkpoint-180000',\n",
    "    'lr': 0.001,\n",
    "    'num_train_epochs': 5,\n",
    "    'seed': 42,\n",
    "    'logging_steps': 10,\n",
    "    'save': '',\n",
    "    'batch_size': 64,\n",
    "    'classes': 2,\n",
    "    'penalization_coeff': 1,\n",
    "    'attention_hops': 1,\n",
    "    'attention_unit': 350\n",
    "}\n",
    "\n",
    "model = Classifier({\n",
    "        'dropout': 0.5,\n",
    "        'attention-unit': 350,\n",
    "        'attention-hops': 1,\n",
    "        'nfc': 512,\n",
    "        'classes': 2,\n",
    "        'model_path': '../checkpoint-180000'\n",
    "})\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(config['model_path'])\n",
    "gpt_args = torch.load(os.path.join(config['model_path'], 'training_args.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trX, vaX, teX, trY, vaY, teY = sst_binary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(trX, trY, tokenizer=tokenizer, block_size=gpt_args.block_size)\n",
    "eval_dataset = CustomDataset(vaX, vaY, tokenizer=tokenizer, block_size=gpt_args.block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/109 [00:00<?, ?it/s]\u001b[A../self_attentive_model.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alphas = self.softmax(penalized_alphas.view(-1, size[1]))  # [bsz*hop, len]\n",
      "\n",
      "Iteration:   1%|          | 1/109 [00:14<26:36, 14.78s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 2/109 [00:28<25:50, 14.49s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 3/109 [00:42<25:04, 14.19s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 4/109 [00:56<25:07, 14.36s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 5/109 [01:13<25:55, 14.95s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 6/109 [01:28<25:48, 15.03s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 7/109 [01:41<24:35, 14.47s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 8/109 [01:57<25:07, 14.93s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 9/109 [02:10<24:06, 14.47s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 10/109 [02:25<24:03, 14.58s/it]\u001b[A\n",
      "Iteration:  10%|█         | 11/109 [02:39<23:37, 14.46s/it]\u001b[A\n",
      "Iteration:  11%|█         | 12/109 [02:55<23:55, 14.80s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "train(config, train_dataset, eval_dataset, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
