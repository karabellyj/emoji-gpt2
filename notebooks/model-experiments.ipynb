{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import emoji\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from config import GPT2EmojiConfig\n",
    "from model import GPT2LMEmojiModel\n",
    "from transformers import GPT2Tokenizer\n",
    "from run_language_modeling import load_and_cache_examples, targets_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    \"gpt2\": (GPT2EmojiConfig, GPT2LMEmojiModel, GPT2Tokenizer),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '../checkpoint-180000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = torch.load(os.path.join(MODEL_PATH, 'training_args.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config_class, model_class, tokenizer_class = MODEL_CLASSES['gpt2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config_class.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizer_class.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_class.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.eval_data_file = '../' + args.eval_data_file\n",
    "eval_dataset = load_and_cache_examples(args, tokenizer, evaluate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15883,   683,   262, 49414,     0,  1312,   760,   334,  3730,   481,\n",
       "        52291])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2811 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "map_token_id_to_target = dict(\n",
    "        zip(tokenizer.encode(list(emoji.UNICODE_EMOJI.keys())), range(0, len(emoji.UNICODE_EMOJI.keys())))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2811 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "map_target_to_token_id = dict(\n",
    "        zip(range(0, len(emoji.UNICODE_EMOJI.keys())), tokenizer.encode(list(emoji.UNICODE_EMOJI.keys())))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_targets_to_tokens(targets):\n",
    "    ids = [map_target_to_token_id[target] for target in targets]\n",
    "    return tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = eval_dataset[0].view(1, -1)\n",
    "labels, mask = targets_mask(inputs, tokenizer, map_token_id_to_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(inputs, labels=labels, inputs_mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_preds = outputs[1].view(inputs.size(1), -1).topk(5, dim=1)[1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[607, 391, 403, 1134, 806],\n",
       " [806, 1134, 1723, 884, 804],\n",
       " [939, 2160, 683, 2239, 1059],\n",
       " [1134, 1936, 2041, 2154, 806],\n",
       " [2154, 1134, 1936, 2182, 2305],\n",
       " [1936, 1982, 2035, 2305, 1134],\n",
       " [1134, 1936, 2041, 1723, 2154],\n",
       " [1936, 1134, 2305, 2035, 793],\n",
       " [1134, 1936, 1982, 2305, 2182],\n",
       " [1936, 2305, 2182, 1134, 1982],\n",
       " [2035, 1982, 1134, 793, 2305]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['make',\n",
       " 'Ä him',\n",
       " 'Ä the',\n",
       " 'Ä happiest',\n",
       " '!',\n",
       " 'Ä i',\n",
       " 'Ä know',\n",
       " 'Ä u',\n",
       " 'Ä guys',\n",
       " 'Ä will',\n",
       " 'â¤']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ğŸ‘ ğŸ‘‡ ğŸ‘‰ ğŸ˜­ ğŸ˜‚', 'make'),\n",
       " ('ğŸ˜‚ ğŸ˜­ ğŸ˜” ğŸ˜³ ğŸ˜¤', 'Ä him'),\n",
       " ('ğŸ ğŸ ğŸ‘‘ ğŸŠ ğŸ”‘', 'Ä the'),\n",
       " ('ğŸ˜­ ğŸ¥º ğŸ˜Œ ğŸ˜ ğŸ˜‚', 'Ä happiest'),\n",
       " ('ğŸ˜ ğŸ˜­ ğŸ¥º ğŸ’– ğŸ’•', '!'),\n",
       " ('ğŸ¥º ğŸ’œ â¤ ğŸ’• ğŸ˜­', 'Ä i'),\n",
       " ('ğŸ˜­ ğŸ¥º ğŸ˜Œ ğŸ˜” ğŸ˜', 'Ä know'),\n",
       " ('ğŸ¥º ğŸ˜­ ğŸ’• â¤ ğŸ˜˜', 'Ä u'),\n",
       " ('ğŸ˜­ ğŸ¥º ğŸ’œ ğŸ’• ğŸ’–', 'Ä guys'),\n",
       " ('ğŸ¥º ğŸ’• ğŸ’– ğŸ˜­ ğŸ’œ', 'Ä will'),\n",
       " ('â¤ ğŸ’œ ğŸ˜­ ğŸ˜˜ ğŸ’•', 'â¤')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(map_targets_to_tokens(pred), text) for pred, text in zip(top5_preds, tokenizer.convert_ids_to_tokens(inputs[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(model, item, topk=5):\n",
    "    inputs = item.view(1, -1)\n",
    "    labels, mask = targets_mask(inputs, tokenizer, map_token_id_to_target)\n",
    "    \n",
    "    outputs = model(inputs, labels=labels, inputs_mask=mask)\n",
    "    topk_preds = outputs[1].view(inputs.size(1), -1).topk(topk, dim=1)[1].tolist()\n",
    "    return topk_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_tokens(inp):\n",
    "    return tokenizer.convert_ids_to_tokens(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ğŸ‘ â¤ ğŸ˜‚ ğŸ˜­ ğŸ˜', 'I'),\n",
       " ('ğŸ˜‚ ğŸ˜­ ğŸ’¯ ğŸ˜ ğŸ˜Œ', 'Ä get'),\n",
       " ('ğŸ˜­ ğŸ˜‚ ğŸ˜© ğŸ˜” ğŸ™„', 'Ä distracted'),\n",
       " ('ğŸ˜‚ ğŸ˜­ ğŸ™ƒ ğŸ™„ ğŸ˜…', 'Ä easily'),\n",
       " ('ğŸ˜‚ ğŸ˜­ ğŸ˜” ğŸ™„ ğŸ™ƒ', 'Ä l'),\n",
       " ('ğŸ˜‚ ğŸ˜­ ğŸ’€ ğŸ¤£ ğŸ˜…', 'ma'),\n",
       " ('ğŸ˜‚ ğŸ˜­ ğŸ’€ ğŸ¤£ ğŸ˜…', 'o'),\n",
       " ('ğŸ˜‚ ğŸ˜­ ğŸ’€ ğŸ˜… ğŸ¤£', 'Ä mainly'),\n",
       " ('ğŸ˜‚ ğŸ˜­ ğŸ˜… ğŸ’€ ğŸ™„', 'Ä because'),\n",
       " ('ğŸ˜‚ ğŸ˜­ ğŸ™ƒ ğŸ˜… ğŸ™„', 'Ä of'),\n",
       " ('ğŸ˜‚ ğŸ˜­ ğŸ˜… ğŸ’€ ğŸ™ƒ', 'Ä music'),\n",
       " ('ğŸ˜‚ ğŸ˜­ ğŸ¤£ ğŸ˜… ğŸ’€', 'ğŸ˜‚')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(map_targets_to_tokens(pred), text) for pred, text in zip(get_preds(model, eval_dataset[4]),ids_to_tokens(eval_dataset[4])) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ğŸ‘ â¤ ğŸ˜‚ ğŸ˜­ ğŸ˜', 'I'),\n",
       " ('ğŸ˜­ ğŸ˜‚ ğŸ˜© ğŸ˜” ğŸ˜Š', 'Ä must'),\n",
       " ('ğŸ˜‚ ğŸ˜Š ğŸ˜… ğŸ‘ ğŸ˜‰', 'Ä say'),\n",
       " ('ğŸ˜‚ ğŸ˜­ ğŸ˜ ğŸ˜ ğŸ˜…', '!!'),\n",
       " ('ğŸ˜‚ ğŸ”¥ ğŸ˜ ğŸ‘ ğŸ˜­', 'Ä So'),\n",
       " ('ğŸ˜‚ ğŸ˜­ ğŸ˜ ğŸ˜… ğŸ˜©', 'oo'),\n",
       " ('ğŸ˜‚ ğŸ˜­ ğŸ˜ ğŸ˜© ğŸ˜Š', 'Ä fl'),\n",
       " ('ğŸ˜‚ ğŸ˜­ ğŸ˜ ğŸ˜Š ğŸ™„', 'ipp'),\n",
       " ('ğŸ˜‚ ğŸ˜­ ğŸ˜ ğŸ˜© ğŸ™„', 'in'),\n",
       " ('ğŸ˜ ğŸ˜­ ğŸ˜‚ ğŸ¥º ğŸ˜Š', 'Ä cute'),\n",
       " ('ğŸ˜ ğŸ˜­ ğŸ’œ ğŸ¥º ğŸ˜‚', '!!!'),\n",
       " ('ğŸŒ¹ ğŸ˜˜ ğŸŒ¼ ğŸ˜Š âœ¨', 'ğŸŒ¹'),\n",
       " ('ğŸŒ¹ ğŸŒ¸ ğŸŒ¼ ğŸŒ» âœ¨', 'ğŸ¦‹'),\n",
       " ('ğŸŒ¹ ğŸŒ¼ ğŸŒ» ğŸŒ¸ ğŸ¦‹', 'ğŸ¦Š'),\n",
       " ('ğŸŒ¹ ğŸŒ¸ ğŸŒ· ğŸŒ¼ ğŸŒ»', 'ğŸŒ¶'),\n",
       " ('ğŸŒ¸ ğŸŒ¹ ğŸŒ» ğŸŒ· ğŸŒ¼', 'ğŸ¦„')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = eval_dataset[44]\n",
    "[(map_targets_to_tokens(pred), text) for pred, text in zip(get_preds(model, data),ids_to_tokens(data)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ğŸ˜‚ ğŸ˜­ ğŸ‘€ ğŸ‘ ğŸ˜', 'Wait'),\n",
       " ('ğŸ˜‚ ğŸ”¥ ğŸ˜ ğŸ‘‡ ğŸ‘€', 'Ä for'),\n",
       " ('ğŸ ğŸ”¥ ğŸ† ğŸ‘‘ ğŸŠ', 'Ä the'),\n",
       " ('ğŸ˜‚ ğŸ˜­ ğŸ˜ ğŸ”¥ ğŸ™', 'Ä mass'),\n",
       " ('ğŸ˜‚ ğŸ˜· ğŸ˜­ ğŸ‘€ ğŸ˜', 'Ä vaccination'),\n",
       " ('ğŸ˜‚ ğŸ˜­ ğŸ‘€ ğŸ™„ ğŸ˜¬', 'Ä campaign'),\n",
       " ('ğŸ˜‚ ğŸ™„ ğŸ˜­ ğŸ‘ ğŸ˜', 'Ä very'),\n",
       " ('ğŸ˜‚ ğŸ˜­ ğŸ‘€ ğŸ™ ğŸ˜·', 'Ä soon'),\n",
       " ('ğŸ‘€ ğŸ˜‚ ğŸ¤£ ğŸ‘‡ ğŸ¤”', 'ğŸ‘€'),\n",
       " ('ğŸ‘‡ ğŸ‘€ ğŸ½ ğŸ¼ ğŸ¾', 'ğŸ‘‚')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = eval_dataset[124]\n",
    "[(map_targets_to_tokens(pred), text) for pred, text in zip(get_preds(model, data),ids_to_tokens(data)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ğŸ˜­ ğŸ˜‚ ğŸ‘ ğŸ™ ğŸ˜”', 'Rest'),\n",
       " ('ğŸ™ ğŸ˜­ ğŸ˜” ğŸ˜‚ ğŸ¥º', 'Ä helps'),\n",
       " ('ğŸ™ ğŸ˜­ ğŸ‘ â¤ ğŸ˜”', 'Ä a'),\n",
       " ('ğŸ™ ğŸ˜­ ğŸ’œ ğŸ¥º â¤', 'Ä lot'),\n",
       " ('ğŸ™ ğŸ˜­ â¤ ğŸ’œ ğŸ˜Š', 'Ä and'),\n",
       " ('ğŸ™ ğŸ’œ ğŸ˜Š â¤ ğŸ’™', 'Ä having'),\n",
       " ('ğŸ™ ğŸ’œ ğŸ‘ ğŸ’• âœ¨', 'Ä quality'),\n",
       " ('ğŸ™ ğŸ’œ ğŸ˜Š ğŸ‘ ğŸ’•', 'Ä time'),\n",
       " ('ğŸ˜Š ğŸ™ ğŸ’œ â¤ ğŸ’™', 'Ä just'),\n",
       " ('ğŸ˜Š ğŸ™ ğŸ’• â¤ âœ¨', 'Ä being'),\n",
       " ('ğŸ’œ ğŸ’• âœ¨ ğŸ’™ â¤', 'Ä yourself'),\n",
       " ('â¤ ğŸ’œ ğŸ˜Š ğŸ’• ğŸ’™', 'Ä and'),\n",
       " ('â¤ ğŸ˜Š ğŸ’œ ğŸ’• ğŸ™', 'Ä not'),\n",
       " ('â¤ ğŸ™ ğŸ’œ ğŸ’™ ğŸ’•', 'Ä M'),\n",
       " ('ğŸ™ ğŸ’• ğŸ’œ ğŸ’™ â¤', 'omm'),\n",
       " ('ğŸ™ ğŸ˜Š ğŸ’œ â¤ ğŸ’•', 'ing'),\n",
       " ('ğŸ™ ğŸ˜Š â¤ ğŸ’œ ğŸ’™', '.'),\n",
       " ('ğŸ–¤ âœ¨ ğŸ’› \\U0001f90d ğŸ’™', 'ğŸ–¤')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = eval_dataset[548]\n",
    "[(map_targets_to_tokens(pred), text) for pred, text in zip(get_preds(model, data),ids_to_tokens(data)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ğŸ‘ ğŸ˜ ğŸ˜­ ğŸ‘ ğŸ¥º', 'good')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_good = torch.tensor(tokenizer.encode('good'))\n",
    "[(map_targets_to_tokens(pred), text) for pred, text in zip(get_preds(model, word_good),ids_to_tokens(word_good)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ğŸ‘ â¤ ğŸ˜‚ ğŸ˜­ ğŸ¥º', 'you'), ('ğŸ¥º â¤ ğŸ’– ğŸ’œ ğŸ˜­', 'Ä are'), ('ğŸ˜‚ ğŸ˜” ğŸ˜­ ğŸ˜³ ğŸ™„', 'Ä mean')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_bad = torch.tensor(tokenizer.encode('you are mean'))\n",
    "[(map_targets_to_tokens(pred), text) for pred, text in zip(get_preds(model, word_bad),ids_to_tokens(word_bad)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1134, 806, 607, 889, 1723],\n",
       " [889, 1134, 1723, 806, 1936],\n",
       " [889, 1134, 2265, 2035, 1723],\n",
       " [889, 1134, 1982, 1936, 2035],\n",
       " [889, 1134, 2035, 1982, 2156],\n",
       " [889, 1982, 2156, 2035, 494],\n",
       " [889, 1982, 2265, 2305, 2181],\n",
       " [889, 1982, 2156, 2265, 2305],\n",
       " [2156, 889, 1982, 2035, 494],\n",
       " [2156, 889, 2305, 2035, 2181],\n",
       " [1982, 2305, 2181, 494, 2035],\n",
       " [2035, 1982, 2156, 2305, 494],\n",
       " [2035, 2156, 1982, 2305, 889],\n",
       " [2035, 889, 1982, 494, 2305],\n",
       " [889, 2305, 1982, 494, 2035],\n",
       " [889, 2156, 1982, 2035, 2305],\n",
       " [889, 2156, 2035, 1982, 494],\n",
       " [465, 2181, 2757, 2379, 494]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = eval_dataset[548]\n",
    "get_preds(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "def get_batch_logits(model, data):\n",
    "    inputs = data.view(1, -1)\n",
    "    labels, mask = targets_mask(inputs, tokenizer, map_token_id_to_target)\n",
    "    \n",
    "    outputs = model(inputs, labels=labels, inputs_mask=mask)\n",
    "    return outputs[1].view(inputs.size(1), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.0006, -7.5493, -9.0351,  ..., -6.0885, -6.6080, -7.2215],\n",
       "        [-5.3994, -9.2103, -9.7454,  ..., -6.3101, -7.9404, -8.5960],\n",
       "        [-3.8682, -7.1625, -7.3015,  ..., -4.6516, -5.0730, -6.3739],\n",
       "        ...,\n",
       "        [-5.5149, -7.9127, -8.5802,  ..., -6.3734, -6.4540, -7.8369],\n",
       "        [-4.4432, -6.7025, -7.3937,  ..., -5.1135, -5.3539, -6.7177],\n",
       "        [-1.6423, -2.7787, -3.0285,  ..., -1.3170, -1.6822, -2.2014]],\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = eval_dataset[548]\n",
    "get_batch_logits(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
