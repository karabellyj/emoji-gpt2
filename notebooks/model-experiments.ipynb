{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import emoji\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from config import GPT2EmojiConfig\n",
    "from model import GPT2LMEmojiModel\n",
    "from transformers import GPT2Tokenizer\n",
    "from run_language_modeling import load_and_cache_examples, targets_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    \"gpt2\": (GPT2EmojiConfig, GPT2LMEmojiModel, GPT2Tokenizer),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '../checkpoint-180000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = torch.load(os.path.join(MODEL_PATH, 'training_args.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config_class, model_class, tokenizer_class = MODEL_CLASSES['gpt2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config_class.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizer_class.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_class.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.eval_data_file = '../' + args.eval_data_file\n",
    "eval_dataset = load_and_cache_examples(args, tokenizer, evaluate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15883,   683,   262, 49414,     0,  1312,   760,   334,  3730,   481,\n",
       "        52291])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2811 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "map_token_id_to_target = dict(\n",
    "        zip(tokenizer.encode(list(emoji.UNICODE_EMOJI.keys())), range(0, len(emoji.UNICODE_EMOJI.keys())))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2811 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "map_target_to_token_id = dict(\n",
    "        zip(range(0, len(emoji.UNICODE_EMOJI.keys())), tokenizer.encode(list(emoji.UNICODE_EMOJI.keys())))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_targets_to_tokens(targets):\n",
    "    ids = [map_target_to_token_id[target] for target in targets]\n",
    "    return tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = eval_dataset[0].view(1, -1)\n",
    "labels, mask = targets_mask(inputs, tokenizer, map_token_id_to_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(inputs, labels=labels, inputs_mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_preds = outputs[1].view(inputs.size(1), -1).topk(5, dim=1)[1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[607, 391, 403, 1134, 806],\n",
       " [806, 1134, 1723, 884, 804],\n",
       " [939, 2160, 683, 2239, 1059],\n",
       " [1134, 1936, 2041, 2154, 806],\n",
       " [2154, 1134, 1936, 2182, 2305],\n",
       " [1936, 1982, 2035, 2305, 1134],\n",
       " [1134, 1936, 2041, 1723, 2154],\n",
       " [1936, 1134, 2305, 2035, 793],\n",
       " [1134, 1936, 1982, 2305, 2182],\n",
       " [1936, 2305, 2182, 1134, 1982],\n",
       " [2035, 1982, 1134, 793, 2305]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['make',\n",
       " 'Ġhim',\n",
       " 'Ġthe',\n",
       " 'Ġhappiest',\n",
       " '!',\n",
       " 'Ġi',\n",
       " 'Ġknow',\n",
       " 'Ġu',\n",
       " 'Ġguys',\n",
       " 'Ġwill',\n",
       " '❤']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('👏 👇 👉 😭 😂', 'make'),\n",
       " ('😂 😭 😔 😳 😤', 'Ġhim'),\n",
       " ('🐐 🐍 👑 🍊 🔑', 'Ġthe'),\n",
       " ('😭 🥺 😌 😍 😂', 'Ġhappiest'),\n",
       " ('😍 😭 🥺 💖 💕', '!'),\n",
       " ('🥺 💜 ❤ 💕 😭', 'Ġi'),\n",
       " ('😭 🥺 😌 😔 😍', 'Ġknow'),\n",
       " ('🥺 😭 💕 ❤ 😘', 'Ġu'),\n",
       " ('😭 🥺 💜 💕 💖', 'Ġguys'),\n",
       " ('🥺 💕 💖 😭 💜', 'Ġwill'),\n",
       " ('❤ 💜 😭 😘 💕', '❤')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(map_targets_to_tokens(pred), text) for pred, text in zip(top5_preds, tokenizer.convert_ids_to_tokens(inputs[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(model, item, topk=5):\n",
    "    inputs = item.view(1, -1)\n",
    "    labels, mask = targets_mask(inputs, tokenizer, map_token_id_to_target)\n",
    "    \n",
    "    outputs = model(inputs, labels=labels, inputs_mask=mask)\n",
    "    topk_preds = outputs[1].view(inputs.size(1), -1).topk(topk, dim=1)[1].tolist()\n",
    "    return topk_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_tokens(inp):\n",
    "    return tokenizer.convert_ids_to_tokens(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('👏 ❤ 😂 😭 😍', 'I'),\n",
       " ('😂 😭 💯 😎 😌', 'Ġget'),\n",
       " ('😭 😂 😩 😔 🙄', 'Ġdistracted'),\n",
       " ('😂 😭 🙃 🙄 😅', 'Ġeasily'),\n",
       " ('😂 😭 😔 🙄 🙃', 'Ġl'),\n",
       " ('😂 😭 💀 🤣 😅', 'ma'),\n",
       " ('😂 😭 💀 🤣 😅', 'o'),\n",
       " ('😂 😭 💀 😅 🤣', 'Ġmainly'),\n",
       " ('😂 😭 😅 💀 🙄', 'Ġbecause'),\n",
       " ('😂 😭 🙃 😅 🙄', 'Ġof'),\n",
       " ('😂 😭 😅 💀 🙃', 'Ġmusic'),\n",
       " ('😂 😭 🤣 😅 💀', '😂')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(map_targets_to_tokens(pred), text) for pred, text in zip(get_preds(model, eval_dataset[4]),ids_to_tokens(eval_dataset[4])) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('👏 ❤ 😂 😭 😍', 'I'),\n",
       " ('😭 😂 😩 😔 😊', 'Ġmust'),\n",
       " ('😂 😊 😅 👍 😉', 'Ġsay'),\n",
       " ('😂 😭 😍 😁 😅', '!!'),\n",
       " ('😂 🔥 😍 👍 😭', 'ĠSo'),\n",
       " ('😂 😭 😍 😅 😩', 'oo'),\n",
       " ('😂 😭 😍 😩 😊', 'Ġfl'),\n",
       " ('😂 😭 😍 😊 🙄', 'ipp'),\n",
       " ('😂 😭 😍 😩 🙄', 'in'),\n",
       " ('😍 😭 😂 🥺 😊', 'Ġcute'),\n",
       " ('😍 😭 💜 🥺 😂', '!!!'),\n",
       " ('🌹 😘 🌼 😊 ✨', '🌹'),\n",
       " ('🌹 🌸 🌼 🌻 ✨', '🦋'),\n",
       " ('🌹 🌼 🌻 🌸 🦋', '🦊'),\n",
       " ('🌹 🌸 🌷 🌼 🌻', '🌶'),\n",
       " ('🌸 🌹 🌻 🌷 🌼', '🦄')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = eval_dataset[44]\n",
    "[(map_targets_to_tokens(pred), text) for pred, text in zip(get_preds(model, data),ids_to_tokens(data)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('😂 😭 👀 👏 😍', 'Wait'),\n",
       " ('😂 🔥 😍 👇 👀', 'Ġfor'),\n",
       " ('🐐 🔥 🍆 👑 🍊', 'Ġthe'),\n",
       " ('😂 😭 😍 🔥 🙏', 'Ġmass'),\n",
       " ('😂 😷 😭 👀 😁', 'Ġvaccination'),\n",
       " ('😂 😭 👀 🙄 😬', 'Ġcampaign'),\n",
       " ('😂 🙄 😭 👍 😁', 'Ġvery'),\n",
       " ('😂 😭 👀 🙏 😷', 'Ġsoon'),\n",
       " ('👀 😂 🤣 👇 🤔', '👀'),\n",
       " ('👇 👀 🏽 🏼 🏾', '👂')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = eval_dataset[124]\n",
    "[(map_targets_to_tokens(pred), text) for pred, text in zip(get_preds(model, data),ids_to_tokens(data)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('😭 😂 👏 🙏 😔', 'Rest'),\n",
       " ('🙏 😭 😔 😂 🥺', 'Ġhelps'),\n",
       " ('🙏 😭 👍 ❤ 😔', 'Ġa'),\n",
       " ('🙏 😭 💜 🥺 ❤', 'Ġlot'),\n",
       " ('🙏 😭 ❤ 💜 😊', 'Ġand'),\n",
       " ('🙏 💜 😊 ❤ 💙', 'Ġhaving'),\n",
       " ('🙏 💜 👍 💕 ✨', 'Ġquality'),\n",
       " ('🙏 💜 😊 👍 💕', 'Ġtime'),\n",
       " ('😊 🙏 💜 ❤ 💙', 'Ġjust'),\n",
       " ('😊 🙏 💕 ❤ ✨', 'Ġbeing'),\n",
       " ('💜 💕 ✨ 💙 ❤', 'Ġyourself'),\n",
       " ('❤ 💜 😊 💕 💙', 'Ġand'),\n",
       " ('❤ 😊 💜 💕 🙏', 'Ġnot'),\n",
       " ('❤ 🙏 💜 💙 💕', 'ĠM'),\n",
       " ('🙏 💕 💜 💙 ❤', 'omm'),\n",
       " ('🙏 😊 💜 ❤ 💕', 'ing'),\n",
       " ('🙏 😊 ❤ 💜 💙', '.'),\n",
       " ('🖤 ✨ 💛 \\U0001f90d 💙', '🖤')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = eval_dataset[548]\n",
    "[(map_targets_to_tokens(pred), text) for pred, text in zip(get_preds(model, data),ids_to_tokens(data)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('👏 😍 😭 👍 🥺', 'good')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_good = torch.tensor(tokenizer.encode('good'))\n",
    "[(map_targets_to_tokens(pred), text) for pred, text in zip(get_preds(model, word_good),ids_to_tokens(word_good)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('👏 ❤ 😂 😭 🥺', 'you'), ('🥺 ❤ 💖 💜 😭', 'Ġare'), ('😂 😔 😭 😳 🙄', 'Ġmean')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_bad = torch.tensor(tokenizer.encode('you are mean'))\n",
    "[(map_targets_to_tokens(pred), text) for pred, text in zip(get_preds(model, word_bad),ids_to_tokens(word_bad)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1134, 806, 607, 889, 1723],\n",
       " [889, 1134, 1723, 806, 1936],\n",
       " [889, 1134, 2265, 2035, 1723],\n",
       " [889, 1134, 1982, 1936, 2035],\n",
       " [889, 1134, 2035, 1982, 2156],\n",
       " [889, 1982, 2156, 2035, 494],\n",
       " [889, 1982, 2265, 2305, 2181],\n",
       " [889, 1982, 2156, 2265, 2305],\n",
       " [2156, 889, 1982, 2035, 494],\n",
       " [2156, 889, 2305, 2035, 2181],\n",
       " [1982, 2305, 2181, 494, 2035],\n",
       " [2035, 1982, 2156, 2305, 494],\n",
       " [2035, 2156, 1982, 2305, 889],\n",
       " [2035, 889, 1982, 494, 2305],\n",
       " [889, 2305, 1982, 494, 2035],\n",
       " [889, 2156, 1982, 2035, 2305],\n",
       " [889, 2156, 2035, 1982, 494],\n",
       " [465, 2181, 2757, 2379, 494]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = eval_dataset[548]\n",
    "get_preds(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "def get_batch_logits(model, data):\n",
    "    inputs = data.view(1, -1)\n",
    "    labels, mask = targets_mask(inputs, tokenizer, map_token_id_to_target)\n",
    "    \n",
    "    outputs = model(inputs, labels=labels, inputs_mask=mask)\n",
    "    return outputs[1].view(inputs.size(1), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.0006, -7.5493, -9.0351,  ..., -6.0885, -6.6080, -7.2215],\n",
       "        [-5.3994, -9.2103, -9.7454,  ..., -6.3101, -7.9404, -8.5960],\n",
       "        [-3.8682, -7.1625, -7.3015,  ..., -4.6516, -5.0730, -6.3739],\n",
       "        ...,\n",
       "        [-5.5149, -7.9127, -8.5802,  ..., -6.3734, -6.4540, -7.8369],\n",
       "        [-4.4432, -6.7025, -7.3937,  ..., -5.1135, -5.3539, -6.7177],\n",
       "        [-1.6423, -2.7787, -3.0285,  ..., -1.3170, -1.6822, -2.2014]],\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = eval_dataset[548]\n",
    "get_batch_logits(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
