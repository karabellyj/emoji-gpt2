{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2811 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import emoji\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from tqdm import tqdm\n",
    "from config import GPT2EmojiConfig\n",
    "from model import GPT2LMEmojiModel\n",
    "from transformers import GPT2Tokenizer\n",
    "from run_language_modeling import load_and_cache_examples, targets_mask\n",
    "from sst_binary import sst_binary\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    \"gpt2\": (GPT2EmojiConfig, GPT2LMEmojiModel, GPT2Tokenizer),\n",
    "}\n",
    "\n",
    "MODEL_PATH = '../checkpoint-180000'\n",
    "\n",
    "args = torch.load(os.path.join(MODEL_PATH, 'training_args.bin'))\n",
    "\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES['gpt2']\n",
    "\n",
    "config = config_class.from_pretrained(MODEL_PATH)\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(MODEL_PATH)\n",
    "\n",
    "model = model_class.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            config=config,\n",
    ")\n",
    "\n",
    "map_target_to_token_id = dict(\n",
    "        zip(range(0, len(emoji.UNICODE_EMOJI.keys())), tokenizer.encode(list(emoji.UNICODE_EMOJI.keys())))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trX, vaX, teX, trY, vaY, teY = sst_binary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_reg_cv(trX, trY, vaX, vaY, teX=None, teY=None, penalty='l1',\n",
    "        C=2**np.arange(-8, 1).astype(np.float), seed=42, solver='liblinear', max_iter=int(1e6)):\n",
    "    scores = []\n",
    "    for i, c in enumerate(C):\n",
    "        model = LogisticRegression(C=c, penalty=penalty, random_state=seed+i, solver=solver, max_iter=max_iter)\n",
    "        model.fit(trX, trY)\n",
    "        score = model.score(vaX, vaY)\n",
    "        scores.append(score)\n",
    "    c = C[np.argmax(scores)]\n",
    "    model = LogisticRegression(C=c, penalty=penalty, random_state=seed+len(C), solver=solver, max_iter=max_iter)\n",
    "    model.fit(trX, trY)\n",
    "    \n",
    "    return model, c, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(X, tokenizer, block_size=512):\n",
    "    return tokenizer.batch_encode_plus(X, add_special_tokens=True, max_length=block_size)[\"input_ids\"]\n",
    "\n",
    "def transform(X, model, tokenizer, args):\n",
    "    X_ids = encode(X, tokenizer, args.block_size)\n",
    "    probas = []\n",
    "    for x in tqdm(X_ids):\n",
    "        outputs = model(torch.tensor(x).unsqueeze(0))\n",
    "        logits = outputs[0].squeeze(0)\n",
    "        \n",
    "        probas.append(logits[-1].tolist())\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6920/6920 [08:00<00:00, 14.40it/s]\n",
      "100%|██████████| 872/872 [01:00<00:00, 14.42it/s]\n",
      "100%|██████████| 1821/1821 [02:09<00:00, 14.09it/s]\n"
     ]
    }
   ],
   "source": [
    "trXt = transform(trX, model, tokenizer, args)\n",
    "vaXt = transform(vaX, model, tokenizer, args)\n",
    "teXt = transform(teX, model, tokenizer, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=0.0625, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=1000000,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l1',\n",
       "                    random_state=51, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " 0.0625,\n",
       " [0.7993119266055045,\n",
       "  0.8073394495412844,\n",
       "  0.8027522935779816,\n",
       "  0.8188073394495413,\n",
       "  0.8279816513761468,\n",
       "  0.8256880733944955,\n",
       "  0.8188073394495413,\n",
       "  0.8084862385321101,\n",
       "  0.7924311926605505])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_with_reg_cv(trXt, trY, vaXt, vaY, teXt, teY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jkarabl/anaconda3/envs/emoji-gpt2/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=0.03125, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=51, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " 0.03125,\n",
       " [0.8325688073394495,\n",
       "  0.8325688073394495,\n",
       "  0.8348623853211009,\n",
       "  0.838302752293578,\n",
       "  0.8371559633027523,\n",
       "  0.8325688073394495,\n",
       "  0.8302752293577982,\n",
       "  0.8348623853211009,\n",
       "  0.8337155963302753])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "trXt_scaled = preprocessing.scale(trXt)\n",
    "vaXt_scaled = preprocessing.scale(vaXt)\n",
    "teXt_scaled = preprocessing.scale(teXt)\n",
    "\n",
    "train_with_reg_cv(trXt_scaled, trY, vaXt_scaled, vaY, teXt_scaled, teY, penalty='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
